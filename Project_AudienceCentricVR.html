<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Audience-centric VR - Julia Ronneberger</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main20240825.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="inner">

							<!-- Logo -->
								<a href="index" class="logo">
									<!--<span class="symbol"><img src="images/logo.svg" alt="" /></span><span class="title">Julia Ronneberger</span> -->
									<span class="title">Julia Ronneberger</span>
								</a>

							<!-- Nav -->
								<nav>
									<ul>
										<li><a href="#menu">Menu</a></li>
									</ul>
								</nav>

						</div>
					</header>

				<!-- Menu -->
				<nav id="menu">
					<h2>Menu</h2>
					<ul>
						<li><a href="index">Home</a></li>
						<li><a href="About">About</a></li>
						<li><a href="index#projects">Projects</a></li>
						<li><a href="Contact">Contact</a></li>
					</ul>
				</nav>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Audience-centric VR - BDFI funded project</h1>

							<span class="image right"><img src="images/BDFI-Project/FirstIdeation.jpg" alt="" /></span>

							<p>
								Last September, <a href="https://www.linkedin.com/in/harry-wilson-78830627a/">Dr Harry Wilson</a>, <a href="https://eirinilampiri.wixsite.com/portfolio">Eirini Lampiri</a> 
								and I were awarded seedcorn funding by the Bristol Digital Futures Institute, to research audience-centric VR.
								We were interested in how one could create immersive experiences, that are accessible to different audiences, by designing combined experiences that engage both
								VR participants and non-VR participants. We set out to answer the research question: 
								<br><br>
								<b><i>How can different modes of communicating between participants inside and outside of VR lead to more playful, accessible experiences?</i></b>

								<br><br>

								<!-- For more information about the research question and the answers we have found so far, have a look at a blog post that Harry wrote about the project: -->

								I am supporting the project as a Creative Techologist. This means that I've been mostly contributing by designing and developing prototypes. 
								These enable us to test different hypothesis, evaluate whether certain interactions are interesting and / or playful and inspire new conversations and ideas.
								The below is a summary of prototypes I have developed as part of this project.

								<br><br>
								If you want to find out more about our early research process and the questions we asked and answered, check out <a href="https://uobtheatre.blogs.bristol.ac.uk/2025/06/19/designing-audience-centric-vr/"><b>Harry's Blog Post</b></a>!
			
								<br>
							</p>


							<h2>Tracked Objects</h2>
							<p>
								One series of experiments involved Vive trackers and trying out different possibilities of connecting one person in VR with one person outside of VR through them.
								We experimented with:
								<ul>
									<li>Trackers being spatial sound sources, where each tracker determined 
										the location of the sound of one instrument of one music track. <a href="https://youtu.be/t111mDuQ4yc">(Video)</a></li>
									<li>Trackers being spotlights in a completely dark environment. <a href="https://youtu.be/xaSu8Pf3M-U">(Video)</a></li>
									<li>Trackers emitting light bubbles, also in a completely dark environment. <a href="https://youtu.be/DH9hWACvrZs">(Video)</a></li>
								</ul>
							</p>


							<div class="image-table">
								<span href="images/BDFI-Project/TrackerTwoPersonExperiment.jpg" class="lightbox image main"><img src="images/BDFI-Project/TrackerTwoPersonExperiment.jpg" alt="" /></span>
								<span href="images/BDFI-Project/TrackersAsSpotlight.png" class="lightbox image main"><img src="images/BDFI-Project/TrackersAsSpotlight.png" alt="" /></span>
								<span href="images/BDFI-Project/TrackersAsLightBubbleEmitters.png" class="lightbox image main"><img src="images/BDFI-Project/TrackersAsLightBubbleEmitters.png" alt="" /></span>
							</div>


							
							<h2>Webcam Feed</h2>
							<p>
								For another series of experiments, we thought about how we could use different scales of environments between the VR person and any participants outside of VR.
								We were thinking about maps that could be drawn outside of VR while influencing the VR environment; 
								or small tracked objects on a stage / in a puppet house, which appear in life-size in VR.
								For this, I developed a few different prototypes that used a webcam feed which influenced the VR environment in different ways:
								<ul>
									<li>Webcam feed as floor texture <a href="https://youtu.be/ZgKQkMSlKvo">(Video)</a></li>
									<li>Webcam feed as floor texture and as a factor in the floor terrain <a href="https://youtu.be/fL6dPxF7ke0">(Video)</a></li>
									<li>Webcam feed as floor texture and a color detection which generates spheres accordingly <a href="https://youtu.be/uqtDeg2-kbs">(Video)</a></li>
									<li>Webcam feed as input for Vuforia, detecting image targets on blocks through a acrylic table setup.</li>
								</ul>
							</p>

							<div class="image-table">
								<span href="images/BDFI-Project/sphereDetectionDev.jpg" class="lightbox image main"><img src="images/BDFI-Project/sphereDetectionDev.jpg" alt="" /></span>
								<span href="images/BDFI-Project/sphereDetectionSetup.jpg" class="lightbox image main"><img src="images/BDFI-Project/sphereDetectionSetup.jpg" alt="" /></span>
								<span href="images/BDFI-Project/ImageTrackingHomeSetup.jpg" class="lightbox image main"><img src="images/BDFI-Project/ImageTrackingHomeSetup.jpg" alt="" /></span>
								<span href="images/BDFI-Project/ImageTrackingTest.jpg" class="lightbox image main"><img src="images/BDFI-Project/ImageTrackingTest.jpg" alt="" /></span>
							</div>

							<h2>First Playtest</h2>
							<p>
								In February, I combined all these experiments into one prototype which we playtested with two groups of students. 
								The prototype uses Vive trackers for life-size and same-space tracked objects and 
								it uses a glass table with image target detection for different-scale and different-space tracked objects.
							</p>

							<div class="image-table">
								<span href="images/BDFI-Project/FirstPlaytestSetupIdeation.jpg" class="lightbox image main"><img src="images/BDFI-Project/FirstPlaytestSetupIdeation.jpg" alt="" /></span>
								<span href="images/BDFI-Project/Playtest1.jpg" class="lightbox image main"><img src="images/BDFI-Project/Playtest1.jpg" alt="" /></span>
								<span href="images/BDFI-Project/Playtest2.jpg" class="lightbox image main"><img src="images/BDFI-Project/Playtest2.jpg" alt="" /></span>
							</div>

							<h2>Further Development</h2>
							<p>
								We got a lot of feedback at the first playtest, and we did our best to address it, while thinking about our past experiments and the wider research questions. 
								One thing we (and the playtest participants) loved about the prototype, was the tabletop setup with tracked objects, that allowed for communication by influencing the virtual scene.
								We focused on that, added a stronger story element, re-worked the visuals and the virtual environment and made the tracking more robust, before showcasing a new version at 
								First Friday at PM Studio. I love the new interactions I got to implent: They included using the image tracker position to uncover a pointcloud in the VR scene and later, to turn that pointcloud into a 
								mesh at the tracker's position.
							</p>

							<div class="image-table">
								<span href="images/BDFI-Project/FirstFridayShowcase/IntroSign.jpg" class="lightbox image main"><img src="images/BDFI-Project/FirstFridayShowcase/IntroSign.jpg" alt="" /></span>
								<span href="images/BDFI-Project/FirstFridayShowcase/VRParticipant.jpg" class="lightbox image main"><img src="images/BDFI-Project/FirstFridayShowcase/VRParticipant.jpg" alt="" /></span>								
								<span href="images/BDFI-Project/FirstFridayShowcase/Storyline.png" class="lightbox image main"><img src="images/BDFI-Project/FirstFridayShowcase/Storyline.png" alt="" /></span>
							</div>

							<div class="image-table">
								<span href="images/BDFI-Project/FirstFridayShowcase/TableTopSetup.jpg" class="lightbox image main"><img src="images/BDFI-Project/FirstFridayShowcase/TableTopSetup.jpg" alt="" /></span>
								<span href="images/BDFI-Project/FirstFridayShowcase/TableTopSetupInAction.jpg" class="lightbox image main"><img src="images/BDFI-Project/FirstFridayShowcase/TableTopSetupInAction.jpg" alt="" /></span>
								<span href="images/BDFI-Project/FirstFridayShowcase/TableTopSetupInAction2.jpg" class="lightbox image main"><img src="images/BDFI-Project/FirstFridayShowcase/TableTopSetupInAction2.jpg" alt="" /></span>
							</div>

						</div>
					</div>

				<!-- Footer -->
				<footer id="footer">
					<div class="inner">
						<section>
						<h2>Contact</h2>
						
						<p>
							<b>Email</b>: <a href="mailto:julia@ronneberger.net">julia@ronneberger.net</a>
							<br><b>Linkedin</b>: <a href="https://www.linkedin.com/in/julia-ronneberger/">Julia Ronneberger</a>
							<br><b>Github</b>: <a href="https://github.com/JuliaRon">JuliaRon</a>
						</p>
					</section>
					</div>
				</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util20240825.js"></script>
			<script src="assets/js/main20240825.js"></script>

	</body>
</html>